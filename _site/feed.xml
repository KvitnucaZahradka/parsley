<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Martin Polacek</title>
    <description>Martin Polacek personal webpage and blog.
</description>
    <link>/parsley/</link>
    <atom:link href="/parsley/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 22 Nov 2016 13:54:45 -0500</pubDate>
    <lastBuildDate>Tue, 22 Nov 2016 13:54:45 -0500</lastBuildDate>
    <generator>Jekyll v3.2.1</generator>
    
      <item>
        <title>Lazy Ions Application</title>
        <description>&lt;h1 id=&quot;application&quot;&gt;Application&lt;/h1&gt;

&lt;p&gt;This is the blog-post about the application called &lt;code class=&quot;highlighter-rouge&quot;&gt;lazy ions&lt;/code&gt;. The 
temporary web-page could be found &lt;a href=&quot;https://glacial-basin-33701.herokuapp.com/about&quot;&gt;here&lt;/a&gt;. The aim of the &lt;code class=&quot;highlighter-rouge&quot;&gt;lazy ions&lt;/code&gt; is 
to analyse the future trendsetters for the new papers released on the 
&lt;a href=&quot;https://arxiv.org&quot;&gt;arXiv&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I am using the Ada - Boosted classifier trained on the set of trendsetter
vs. non-trendsetter papers and metadata downloaded from &lt;a href=&quot;https://arxiv.org&quot;&gt;arXiv&lt;/a&gt;. The trendsetters 
are defined as the most cited authors in the period of last five years in 
certain area of research. The non-trendsetters are the authors that are 
not trendsetters but are still producing papers in given field.&lt;/p&gt;

&lt;p&gt;My application &lt;code class=&quot;highlighter-rouge&quot;&gt;lazy ions&lt;/code&gt; so far works only for &lt;code class=&quot;highlighter-rouge&quot;&gt;hep-th&lt;/code&gt; field on arXiv.
I the future other areas will be included as well. You can check &lt;code class=&quot;highlighter-rouge&quot;&gt;lazy ions&lt;/code&gt; 
code on &lt;a href=&quot;https://github.com/KvitnucaZahradka/LAZY_IONS/blob/master/Document.py&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;hep-th&lt;/code&gt; papers predicted to be trendsetters by my application:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/hep-th/9108012v1&quot;&gt;paper 1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/hep-th/9108012v1&quot;&gt;paper 2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/hep-th/9108003v1&quot;&gt;paper 3&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/hep-th/9108013v1&quot;&gt;paper 4&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/hep-th/9109006v2&quot;&gt;paper 5&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/hep-th/9109027v1&quot;&gt;paper 6&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;application-details&quot;&gt;Application details&lt;/h1&gt;

&lt;p&gt;The whole project is developed in PyCharm in conjunction with live script
run in Jupyther. The project consists of four main steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Getting raw data from arXiv&lt;/li&gt;
  &lt;li&gt;Exploratory analysis&lt;/li&gt;
  &lt;li&gt;Feature engineering, feature selection and Model building&lt;/li&gt;
  &lt;li&gt;Model evaluation and prediction&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I addressed the points &lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;1.&lt;/code&gt;&lt;/strong&gt; to &lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;4.&lt;/code&gt;&lt;/strong&gt; by creating the &lt;a href=&quot;https://github.com/KvitnucaZahradka/LAZY_IONS/blob/master/Document.py&quot;&gt;Python module&lt;/a&gt;. The module
consists of five classes:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;class &lt;strong&gt;Data&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;class &lt;strong&gt;GetLatestData&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;class &lt;strong&gt;Document&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;class &lt;strong&gt;Learn&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;class &lt;strong&gt;Predict&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The following diagram describes the interactions between classes and between classes
and environment.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://kvitnucazahradka.github.io/parsley/pictures/lazy_ions_diagram.JPG&quot; alt=&quot;SF GG&quot; width=&quot;555px&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;exploratory-analysis&quot;&gt;Exploratory analysis&lt;/h2&gt;
&lt;p&gt;In my exploration I looked at various preliminary measures of trendsetting
and non trendsetting papers. One interesting observation was that the 
&lt;em&gt;Dale-Chall&lt;/em&gt; readability of “trendsetting” papers is quite different than 
 the readability of “non-trendsetting” papers. I got the following distributions
 of readabilities:&lt;/p&gt;

&lt;p&gt;For trending papers:
&lt;img src=&quot;https://kvitnucazahradka.github.io/parsley/pictures/trendDaleChall.JPG&quot; alt=&quot;SF GG&quot; width=&quot;555px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For non-trending papers:
&lt;img src=&quot;https://kvitnucazahradka.github.io/parsley/pictures/trendDaleChall_notTrending.JPG&quot; alt=&quot;SF GG&quot; width=&quot;555px&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;feature-engineering-feature-selection-and-model-building&quot;&gt;Feature engineering, feature selection and Model building&lt;/h2&gt;

&lt;p&gt;Having downloaded the meta-data and actual pdf files from arXiv for trendsetters
and non-trendsetters. I split the trendsetting papers to the papers belonging to 
the top trendsetting authors (the upper five per-cent). I call that set &lt;strong&gt;a golden standard&lt;/strong&gt;.
I have done that to have base to which I can calculate the &lt;strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Cosine_similarity&quot;&gt;cosine similarities&lt;/a&gt;&lt;/strong&gt; of the 
rest of the papers to the &lt;strong&gt;golden standard&lt;/strong&gt;. By this I engineered the global feature for each article. Moreover for each 
paper I calculated the local features (meaning related to the paper itself regardless the corpus). 
The following summarises all features calculated for each paper. By selecting following
features I wanted to catch the formal structure together with factual structure
of papers:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Cosine_similarity&quot;&gt;cosine similarities&lt;/a&gt; to the &lt;strong&gt;golden standard&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;equal sign densities of text&lt;/li&gt;
  &lt;li&gt;equal sign densities of abstract&lt;/li&gt;
  &lt;li&gt;Dale-Chall readabilities of text&lt;/li&gt;
  &lt;li&gt;Dale-Chall readabilities of abstract&lt;/li&gt;
  &lt;li&gt;number of lines density of text&lt;/li&gt;
  &lt;li&gt;number of lines density of abstract&lt;/li&gt;
  &lt;li&gt;length of the text&lt;/li&gt;
  &lt;li&gt;length of the abstract&lt;/li&gt;
  &lt;li&gt;pure text density of text&lt;/li&gt;
  &lt;li&gt;pure text density of abstract&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Note, by pure text density ((num. of words in pure text / (num. of words in the orig. text))) I mean 
the density of cleaned up text (text consisting only of factual information nouns).&lt;/p&gt;

&lt;h2 id=&quot;model-evaluation-and-prediction&quot;&gt;Model evaluation and prediction&lt;/h2&gt;
&lt;p&gt;The aim of this project was to train the classifier. I picked the Ada-Boosted 
classifier. The reason was that I still had not enormously huge data set. 
So, in this project the scalability was not an issue. The Ada-Boost is considered
the best &lt;strong&gt;out of box&lt;/strong&gt; classifier. The &lt;a href=&quot;https://en.wikipedia.org/wiki/AdaBoost&quot;&gt;creators&lt;/a&gt; got  &lt;strong&gt;Gödel Prize&lt;/strong&gt; for this algorithm.&lt;/p&gt;

&lt;p&gt;Moreover I used this algorithm before in the work for the medical startup REVON.&lt;/p&gt;

&lt;p&gt;I trained the &lt;a href=&quot;http://scikit-learn.org/stable/&quot;&gt;scikit-learn&lt;/a&gt; version of the Ada_Boost classifier with a grid search.
I split my data into three groups &lt;strong&gt;training&lt;/strong&gt; (70 per-cent) &lt;strong&gt;testing&lt;/strong&gt; (20 per-cent)
&lt;strong&gt;validation&lt;/strong&gt; (10 per-cent).&lt;/p&gt;

&lt;p&gt;After training I got the classifier with accuracy on validation set to be around 67 per-cent. 
By that classifier I predicted the papers whose links you can find at the 
top of the website. The prediction has been made on the most recent &lt;code class=&quot;highlighter-rouge&quot;&gt;hep-th&lt;/code&gt; papers 
from arXiv.&lt;/p&gt;

&lt;h2 id=&quot;future-work-proposal&quot;&gt;Future work proposal&lt;/h2&gt;

&lt;p&gt;For the future work I propose the following:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Get more data with precise number of citations for each paper&lt;/li&gt;
  &lt;li&gt;More model space exploration&lt;/li&gt;
  &lt;li&gt;Extend the algorithm for different (other than &lt;code class=&quot;highlighter-rouge&quot;&gt;hep-th&lt;/code&gt;) arXiv sections&lt;/li&gt;
  &lt;li&gt;Extend the algorithm for other media (other than arXiv)&lt;/li&gt;
  &lt;li&gt;Introduce the online learning, with proper implementation of exploration vs. exploitation&lt;/li&gt;
  &lt;li&gt;Personalised version&lt;/li&gt;
  &lt;li&gt;Use the algorithm as trendsetting-nes estimator for authors&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Mon, 14 Nov 2016 10:04:44 -0500</pubDate>
        <link>/parsley/welcome/</link>
        <guid isPermaLink="true">/parsley/welcome/</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
      </item>
    
  </channel>
</rss>
